import streamlit as st
import pandas as pd
import requests
import time
import concurrent.futures
import pytz
from datetime import datetime
import io
import ftplib
import traceback

# === è¨­å®š ===
JST = pytz.timezone("Asia/Tokyo")

ROOM_LIST_URL = "https://mksoul-pro.com/showroom/file/room_list.csv"
FTP_FILE_PATH = "/mksoul-pro.com/showroom/file/event_database.csv"

EVENT_ID_START = 33000
EVENT_ID_END = 41000  # âœ… ãƒ†ã‚¹ãƒˆç¯„å›²ï¼ˆæ®µéšçš„ã«åºƒã’ã¦OKï¼‰
MAX_WORKERS = 10
SAVE_INTERVAL = 500  # âœ… é »åº¦ã‚’æ¸›ã‚‰ã—ã¦å®‰å®šåŒ–

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
}

# === FTPå‡¦ç† ===
def ftp_upload(file_path, content_bytes, retry=3):
    """FTPã‚µãƒ¼ãƒãƒ¼ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    ftp_host = st.secrets["ftp"]["host"]
    ftp_user = st.secrets["ftp"]["user"]
    ftp_pass = st.secrets["ftp"]["password"]

    for attempt in range(1, retry + 1):
        try:
            with ftplib.FTP(ftp_host, timeout=20) as ftp:
                ftp.login(ftp_user, ftp_pass)
                with io.BytesIO(content_bytes) as f:
                    ftp.storbinary(f"STOR {file_path}", f)
            return True
        except Exception as e:
            if attempt < retry:
                st.warning(f"âš ï¸ FTPæ¥ç¶šå¤±æ•—ï¼ˆ{attempt}/{retry}ï¼‰â†’ 3ç§’å¾Œãƒªãƒˆãƒ©ã‚¤ä¸­...")
                time.sleep(3)
            else:
                st.error(f"ğŸš¨ FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                return False


def ftp_download(file_path):
    """FTPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯Noneï¼‰"""
    ftp_host = st.secrets["ftp"]["host"]
    ftp_user = st.secrets["ftp"]["user"]
    ftp_pass = st.secrets["ftp"]["password"]
    try:
        with ftplib.FTP(ftp_host, timeout=20) as ftp:
            ftp.login(ftp_user, ftp_pass)
            buffer = io.BytesIO()
            ftp.retrbinary(f"RETR {file_path}", buffer.write)
            buffer.seek(0)
            return buffer.getvalue().decode("utf-8-sig")
    except Exception:
        return None


# === ãƒ‡ãƒ¼ã‚¿å–å¾—ç³»é–¢æ•° ===
def fetch_room_list_for_event(event_id):
    """ã‚¤ãƒ™ãƒ³ãƒˆã«å‚åŠ ã—ã¦ã„ã‚‹ãƒ«ãƒ¼ãƒ ã‚’å–å¾—"""
    all_rooms = []
    for page in range(1, 31):  # æœ€å¤§30ãƒšãƒ¼ã‚¸
        url = f"https://www.showroom-live.com/api/event/room_list?event_id={event_id}&p={page}"
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            if res.status_code != 200:
                break
            data = res.json()
            rooms = data.get("list", [])
            if not rooms:
                break
            all_rooms.extend(rooms)
            time.sleep(0.05)
        except Exception:
            break
    return all_rooms


def fetch_event_detail(event_id, room_id):
    """ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ã‚’å–å¾—ï¼ˆcontribution_ranking APIï¼‰"""
    url = f"https://www.showroom-live.com/api/event/contribution_ranking?event_id={event_id}&room_id={room_id}"
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        if res.status_code == 200:
            data = res.json()
            event = data.get("event", {})
            return {
                "event_name": event.get("event_name"),
                "started_at": event.get("started_at"),
                "ended_at": event.get("ended_at"),
                "event_url": event.get("event_url"),
                "event_image": event.get("image"),
            }
    except Exception:
        pass
    return {}


# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
def fetch_and_merge_event_data():
    df_rooms = pd.read_csv(ROOM_LIST_URL, dtype=str)
    df_rooms["ãƒ«ãƒ¼ãƒ ID"] = df_rooms["ãƒ«ãƒ¼ãƒ ID"].astype(str)
    managed_rooms = df_rooms.set_index("ãƒ«ãƒ¼ãƒ ID")

    existing_csv = ftp_download(FTP_FILE_PATH)
    if existing_csv:
        df_existing = pd.read_csv(io.StringIO(existing_csv), dtype=str)
    else:
        df_existing = pd.DataFrame()

    all_records = []
    event_ids = list(range(EVENT_ID_START, EVENT_ID_END + 1))
    total = len(event_ids)
    progress = st.progress(0)
    start_time = time.time()

    def fmt_time(ts):
        if not ts:
            return ""
        try:
            return datetime.fromtimestamp(int(ts), JST).strftime("%Y/%m/%d %H:%M")
        except Exception:
            return ""

    def process_event(event_id):
        event_records = []
        room_list = fetch_room_list_for_event(event_id)
        for r in room_list:
            rid = str(r.get("room_id"))
            if rid not in managed_rooms.index:
                continue
            entry = r.get("event_entry", {})
            rank = r.get("rank") or "-"
            point = r.get("point") or 0
            quest_level = entry.get("quest_level", 0)
            detail = fetch_event_detail(event_id, rid)
            if not detail:
                continue
            rec = {
                "PRå¯¾è±¡": "",
                "ãƒ©ã‚¤ãƒãƒ¼å": managed_rooms.loc[rid, "ãƒ«ãƒ¼ãƒ å"],
                "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID": managed_rooms.loc[rid, "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"],
                "ã‚¤ãƒ™ãƒ³ãƒˆå": detail.get("event_name"),
                "é–‹å§‹æ—¥æ™‚": fmt_time(detail.get("started_at")),
                "çµ‚äº†æ—¥æ™‚": fmt_time(detail.get("ended_at")),
                "é †ä½": rank,
                "ãƒã‚¤ãƒ³ãƒˆ": point,
                "å‚™è€ƒ": "",
                "ç´ä»˜ã‘": "â—‹",
                "URL": detail.get("event_url"),
                "ãƒ¬ãƒ™ãƒ«": quest_level,
                "event_id": str(event_id),
                "ãƒ«ãƒ¼ãƒ ID": rid,
                "ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰": detail.get("event_image"),
            }
            event_records.append(rec)
        return event_records

    processed_count = 0  # âœ… å‡¦ç†å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚«ã‚¦ãƒ³ã‚¿

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_id = {executor.submit(process_event, eid): eid for eid in event_ids}
        for future in concurrent.futures.as_completed(future_to_id):
            eid = future_to_id[future]
            try:
                records = future.result()
                all_records.extend(records)
            except Exception:
                st.warning(f"âš ï¸ event_id={eid} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ\n{traceback.format_exc()}")

            processed_count += 1
            progress.progress(processed_count / total)

            # âœ… ã‚«ã‚¦ãƒ³ã‚¿ãƒ™ãƒ¼ã‚¹ã§é€”ä¸­ä¿å­˜ã‚’å®Ÿè¡Œ
            if processed_count % SAVE_INTERVAL == 0 and all_records:
                df_partial = pd.DataFrame(all_records)
                merged = pd.concat([df_existing, df_partial], ignore_index=True)
                merged.drop_duplicates(subset=["event_id", "ãƒ«ãƒ¼ãƒ ID"], keep="last", inplace=True)
                merged.sort_values("event_id", ascending=False, inplace=True)
                csv_bytes = merged.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
                ok = ftp_upload(FTP_FILE_PATH, csv_bytes)
                timestamp = datetime.now(JST).strftime("%H:%M:%S")
                if ok:
                    st.info(f"ğŸ’¾ {timestamp} é€”ä¸­ä¿å­˜å®Œäº† ({processed_count}/{total})")
                else:
                    st.warning(f"âš ï¸ {timestamp} é€”ä¸­ä¿å­˜ã‚¹ã‚­ãƒƒãƒ—ï¼ˆFTPå¤±æ•—ï¼‰")


    # å®Œå…¨ä¿å­˜
    if all_records:
        df_new = pd.DataFrame(all_records)
        merged = pd.concat([df_existing, df_new], ignore_index=True)
        merged.drop_duplicates(subset=["event_id", "ãƒ«ãƒ¼ãƒ ID"], keep="last", inplace=True)
        merged.sort_values("event_id", ascending=False, inplace=True)
        csv_bytes = merged.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        ftp_upload(FTP_FILE_PATH, csv_bytes)
        st.success("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†ï¼")

        st.download_button(
            label="ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼‰",
            data=csv_bytes,
            file_name=f"event_database_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

    elapsed = time.time() - start_time
    st.write(f"â±ï¸ çµŒéæ™‚é–“: {elapsed/60:.1f} åˆ†")


# === Streamlitç”»é¢ ===
def main():
    st.title("ğŸ¯ SHOWROOM ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ãƒ„ãƒ¼ãƒ«")
    st.caption("ä¸¦åˆ—å–å¾—ãƒ»é€²æ—è¡¨ç¤ºãƒ»FTPãƒªãƒˆãƒ©ã‚¤å¯¾å¿œç‰ˆ")

    if st.button("ğŸš€ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ï¼ˆå®Ÿè¡Œï¼‰"):
        fetch_and_merge_event_data()


if __name__ == "__main__":
    main()
