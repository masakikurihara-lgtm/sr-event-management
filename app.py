import streamlit as st
import requests
import pandas as pd
import io
import time
import pytz
import ftplib
from datetime import datetime

# ====== å®šæ•°å®šç¾© ======
JST = pytz.timezone("Asia/Tokyo")

ROOM_LIST_URL = "https://mksoul-pro.com/showroom/file/room_list.csv"
EVENT_ARCHIVE_URL = "https://mksoul-pro.com/showroom/file/sr-event-archive.csv"
EVENT_DATABASE_PATH = "/mksoul-pro.com/showroom/file/event_database.csv"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
}

#EVENT_ID_START = 30000
EVENT_ID_START = 40291
#EVENT_ID_END = 41000
EVENT_ID_END = 40292
ENTRY_CUTOFF = datetime(2023, 8, 18, 18, 0, tzinfo=JST)

# ====== FTPãƒ˜ãƒ«ãƒ‘ãƒ¼ ======
def ftp_upload(file_path, content_bytes):
    ftp_host = st.secrets["ftp"]["host"]
    ftp_user = st.secrets["ftp"]["user"]
    ftp_pass = st.secrets["ftp"]["password"]
    with ftplib.FTP(ftp_host) as ftp:
        ftp.login(ftp_user, ftp_pass)
        with io.BytesIO(content_bytes) as f:
            ftp.storbinary(f"STOR {file_path}", f)


def ftp_download(file_path):
    ftp_host = st.secrets["ftp"]["host"]
    ftp_user = st.secrets["ftp"]["user"]
    ftp_pass = st.secrets["ftp"]["password"]
    with ftplib.FTP(ftp_host) as ftp:
        ftp.login(ftp_user, ftp_pass)
        buffer = io.BytesIO()
        try:
            ftp.retrbinary(f"RETR {file_path}", buffer.write)
            buffer.seek(0)
            return buffer.getvalue().decode("utf-8-sig")
        except Exception:
            return None


# ====== ã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ æƒ…å ±åé›† ======
def fetch_and_merge_event_data():
    # --- å„ç¨®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
    df_rooms = pd.read_csv(ROOM_LIST_URL, dtype=str)
    df_archive = pd.read_csv(EVENT_ARCHIVE_URL, dtype=str)

    room_ids = set(df_rooms["ãƒ«ãƒ¼ãƒ ID"].astype(str))
    target_records = []

    st.info("ğŸ“¡ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ä¸­...")
    total_checked = 0

    for eid in range(EVENT_ID_START, EVENT_ID_END + 1):
        for p in range(1, 31):
            url = f"https://www.showroom-live.com/api/event/room_list?event_id={eid}&p={p}"
            res = requests.get(url, headers=HEADERS, timeout=10)
            total_checked += 1
            if res.status_code != 200:
                break
            data = res.json()
            entries = data.get("list", [])
            if not entries:
                break

            for r in entries:
                rid = str(r.get("room_id"))
                entry_data = r.get("event_entry", {})
                entried_at = entry_data.get("entried_at", 0)
                if not rid or rid not in room_ids:
                    continue
                # --- æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ ---
                try:
                    entry_dt = datetime.fromtimestamp(entried_at, JST)
                    if entry_dt < ENTRY_CUTOFF:
                        continue
                except Exception:
                    continue

                target_records.append({
                    "room_id": rid,
                    "event_id": eid,
                    "rank": r.get("rank"),
                    "point": r.get("point") or 0,
                    "quest_level": entry_data.get("quest_level", 0),
                    "entried_at": entry_dt.strftime("%Y-%m-%d %H:%M:%S")
                })

            if not data.get("next_page"):
                break
            time.sleep(0.05)

    if not target_records:
        st.warning("è©²å½“ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    df_api = pd.DataFrame(target_records)

    # --- ã‚¤ãƒ™ãƒ³ãƒˆåãªã©ã‚’ãƒãƒ¼ã‚¸ ---
    df_api["event_id"] = df_api["event_id"].astype(str)
    df_archive["event_id"] = df_archive["event_id"].astype(str)

    merged = (
        df_api.merge(df_rooms, left_on="room_id", right_on="ãƒ«ãƒ¼ãƒ ID", how="left")
              .merge(df_archive, on="event_id", how="left")
    )

    # --- ğŸ”§ åˆ—åã‚’æœ€çµ‚å‡ºåŠ›ç”¨ã«å¤‰æ› ---
    merged = merged.rename(columns={
        "room_name": "ãƒ©ã‚¤ãƒãƒ¼å",        # å¿µã®ãŸã‚æ®‹ã™ï¼ˆãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ï¼‰
        "ãƒ«ãƒ¼ãƒ å": "ãƒ©ã‚¤ãƒãƒ¼å",
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID": "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID",
        "event_name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
        "started_at": "é–‹å§‹æ—¥æ™‚",
        "ended_at": "çµ‚äº†æ—¥æ™‚",
        "rank": "é †ä½",
        "point": "ãƒã‚¤ãƒ³ãƒˆ",
        "quest_level": "ãƒ¬ãƒ™ãƒ«"
    })

    # --- ğŸ”§ å›ºå®šåˆ—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ ï¼‰ ---
    for col in ["PRå¯¾è±¡", "ç´ä»˜ã‘", "URL"]:
        if col not in merged.columns:
            merged[col] = ""

    # --- ğŸ”§ è¡¨ç¤ºé †ã‚’å®‰å…¨ã«åˆ¶å¾¡ ---
    expected_cols = [
        "PRå¯¾è±¡", "ãƒ©ã‚¤ãƒãƒ¼å", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ã‚¤ãƒ™ãƒ³ãƒˆå", "é–‹å§‹æ—¥æ™‚", "çµ‚äº†æ—¥æ™‚",
        "é †ä½", "ãƒã‚¤ãƒ³ãƒˆ", "ç´ä»˜ã‘", "URL", "ãƒ¬ãƒ™ãƒ«", "event_id"
    ]
    available_cols = [c for c in expected_cols if c in merged.columns]
    merged = merged[available_cols]

    st.success("âœ… ãƒãƒ¼ã‚¸ã¨åˆ—æ•´å½¢ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    st.write("ğŸ“Š merged.shape:", merged.shape)



# ====== Streamlit UI ======
def main():
    st.title("ğŸ¯ SHOWROOM ã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°")

    if st.button("ğŸš€ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°"):
        with st.spinner("ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’åé›†ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰"):
            df = fetch_and_merge_event_data()

            if not df.empty:
                st.success(f"âœ… {len(df)}ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")

                # --- æ—¢å­˜DBã‚’èª­ã¿è¾¼ã‚“ã§çµ±åˆ ---
                existing_csv = ftp_download(EVENT_DATABASE_PATH)
                if existing_csv:
                    df_existing = pd.read_csv(io.StringIO(existing_csv), dtype=str)
                    merged_df = pd.concat([df_existing, df], ignore_index=True)
                    merged_df.drop_duplicates(subset=["event_id", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"], keep="last", inplace=True)
                else:
                    merged_df = df

                # --- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
                csv_bytes = merged_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
                ftp_upload(EVENT_DATABASE_PATH, csv_bytes)

                st.download_button(
                    "ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ›´æ–°å¾ŒCSVï¼‰",
                    data=csv_bytes,
                    file_name=f"event_database_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )

                st.dataframe(df)

            else:
                st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if __name__ == "__main__":
    main()
