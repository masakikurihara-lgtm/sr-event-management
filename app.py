# app.py (æ”¹è‰¯ç‰ˆ)
import streamlit as st
import requests
import pandas as pd
import io
import time
import ftplib
import socket
import traceback
from datetime import datetime
import pytz
import concurrent.futures
from typing import List, Dict, Any, Optional

JST = pytz.timezone("Asia/Tokyo")

API_ROOM_LIST = "https://www.showroom-live.com/api/event/room_list"
API_CONTRIBUTION = "https://www.showroom-live.com/api/event/contribution_ranking"
ROOM_LIST_URL = "https://mksoul-pro.com/showroom/file/room_list.csv"
ARCHIVE_URL = "https://mksoul-pro.com/showroom/file/sr-event-archive.csv"

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; mksoul-bot/1.0)"}

# FTP info should be placed in Streamlit Secrets:
# [ftp]
# host = "ftp11.gmoserver.jp"
# user = "sd0866487@gmoserver.jp"
# password = "..."
DEFAULT_FTP_FALLBACK = {"host": None, "user": None, "password": None}

# ---------- ãƒ˜ãƒ«ãƒ‘ãƒ¼ ----------
def http_get_json(url: str, params: dict = None, retries: int = 3, timeout: float = 12.0, backoff: float = 0.6):
    """å …ç‰¢ãª GET -> JSONã€‚429ã‚„æ¥ç¶šã‚¨ãƒ©ãƒ¼ã«å¯¾ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€‚å¤±æ•—æ™‚ã¯ None ã‚’è¿”ã™ã€‚"""
    for attempt in range(retries):
        try:
            r = requests.get(url, headers=HEADERS, params=params, timeout=timeout)
            # 429 ã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã£ã½ã„ã®ã§å°‘ã—é•·ã‚ã«å¾…ã¤
            if r.status_code == 429:
                wait = backoff * (attempt + 1) * 2
                time.sleep(wait)
                continue
            if r.status_code == 200:
                try:
                    return r.json()
                except ValueError:
                    return None
            # 404 -> å­˜åœ¨ã—ãªã„
            if r.status_code in (404, 410):
                return None
            # ãã®ä»–ã¯ä¸€æ—¦ãƒªãƒˆãƒ©ã‚¤ã®å¯¾è±¡ã«ã™ã‚‹
            time.sleep(backoff * (attempt + 1))
        except (requests.RequestException, socket.timeout) as e:
            time.sleep(backoff * (attempt + 1))
            continue
    return None

def fmt_time(ts) -> str:
    """Unixç§’ -> 'YYYY/MM/DD HH:MM' (JST) ã¾ãŸã¯ç©ºæ–‡å­—"""
    try:
        if ts is None:
            return ""
        ts = int(ts)
        if ts > 20000000000:
            ts = ts // 1000
        dt = datetime.fromtimestamp(ts, JST)
        return dt.strftime("%Y/%m/%d %H:%M")
    except Exception:
        return ""

def ftp_upload_bytes(file_path: str, content_bytes: bytes, retries: int = 2):
    ftp_host = st.secrets.get("ftp", {}).get("host") or DEFAULT_FTP_FALLBACK["host"]
    ftp_user = st.secrets.get("ftp", {}).get("user") or DEFAULT_FTP_FALLBACK["user"]
    ftp_pass = st.secrets.get("ftp", {}).get("password") or DEFAULT_FTP_FALLBACK["password"]
    if not ftp_host or not ftp_user:
        raise RuntimeError("FTPæƒ…å ±ãŒ st.secrets['ftp'] ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
    last_err = None
    for i in range(retries):
        try:
            with ftplib.FTP(ftp_host, timeout=30) as ftp:
                ftp.login(ftp_user, ftp_pass)
                with io.BytesIO(content_bytes) as bf:
                    bf.seek(0)
                    ftp.storbinary(f"STOR {file_path}", bf)
            return True
        except Exception as e:
            last_err = e
            time.sleep(1 + i)
    raise last_err

# ---------- ç®¡ç†ãƒ«ãƒ¼ãƒ  / ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–èª­ã¿è¾¼ã¿ ----------
@st.cache_data(ttl=3600)
def load_managed_rooms(url: str) -> pd.DataFrame:
    """room_list.csv ã‚’å–å¾—ã—ã¦ 'ãƒ«ãƒ¼ãƒ ID' ã‚’ index ã«ã—ãŸ DataFrame ã‚’è¿”ã™"""
    r = requests.get(url, headers=HEADERS, timeout=15)
    r.raise_for_status()
    txt = r.content.decode("utf-8-sig")
    # å¯èƒ½ãªã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šã§èª­ã¿ã€ç„¡ã‘ã‚Œã° headerless æƒ³å®š
    try:
        df = pd.read_csv(io.StringIO(txt))
        # è‡ªå‹•çš„ã«åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
        cols = df.columns.tolist()
        rename = {}
        for c in cols:
            lc = str(c).lower()
            if "room" in lc and "id" in lc:
                rename[c] = "ãƒ«ãƒ¼ãƒ ID"
            elif ("name" in lc and ("room" in lc or "ãƒ«ãƒ¼ãƒ " in str(c))) or "ãƒ«ãƒ¼ãƒ å" in str(c):
                rename[c] = "ãƒ«ãƒ¼ãƒ å"
            elif "account" in lc or "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ" in str(c):
                rename[c] = "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"
            elif "url" in lc:
                rename[c] = "ãƒ«ãƒ¼ãƒ URL"
        if rename:
            df.rename(columns=rename, inplace=True)
    except Exception:
        df = pd.read_csv(io.StringIO(txt), header=None)
        col_map = {}
        if df.shape[1] >= 1: col_map[0] = "ãƒ«ãƒ¼ãƒ ID"
        if df.shape[1] >= 2: col_map[1] = "ãƒ«ãƒ¼ãƒ å"
        if df.shape[1] >= 3: col_map[2] = "ãƒ«ãƒ¼ãƒ URL"
        if df.shape[1] >= 4: col_map[3] = "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"
        df.rename(columns=col_map, inplace=True)
    if "ãƒ«ãƒ¼ãƒ ID" not in df.columns:
        return pd.DataFrame(columns=["ãƒ«ãƒ¼ãƒ ID"])
    df["ãƒ«ãƒ¼ãƒ ID"] = df["ãƒ«ãƒ¼ãƒ ID"].astype(str).str.strip()
    # ç©ºã®ãƒ«ãƒ¼ãƒ IDè¡Œã¯é™¤å»
    df = df[df["ãƒ«ãƒ¼ãƒ ID"].str.strip() != ""].copy()
    df = df.set_index("ãƒ«ãƒ¼ãƒ ID", drop=False)
    return df

@st.cache_data(ttl=3600)
def load_event_archive(url: str) -> Dict[str, dict]:
    """sr-event-archive.csv ã‚’å–å¾—ã—ã¦ event_id -> dict ã®ãƒãƒƒãƒ—ã‚’è¿”ã™ï¼ˆè£œå¡«ç”¨ï¼‰"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=15)
        r.raise_for_status()
        txt = r.content.decode("utf-8-sig")
        df = pd.read_csv(io.StringIO(txt), dtype=str)
        df["event_id"] = df["event_id"].apply(lambda v: str(v).strip() if pd.notna(v) else "")
        df = df.dropna(subset=["event_id"])
        out = {}
        for _, row in df.iterrows():
            eid = str(row["event_id"])
            out[eid] = {
                "event_name": row.get("event_name"),
                "started_at": int(float(row["started_at"])) if pd.notna(row.get("started_at")) else None,
                "ended_at": int(float(row["ended_at"])) if pd.notna(row.get("ended_at")) else None,
                "image_m": row.get("image_m"),
                "event_url_key": row.get("event_url_key"),
                "show_ranking": row.get("show_ranking")
            }
        return out
    except Exception:
        return {}

# ---------- room_list ãƒšãƒ¼ã‚¸å–å¾—ï¼ˆå …ç‰¢åŒ–ï¼‰ ----------
def fetch_all_room_entries(event_id: int, max_pages: int = 500, sleep_between_pages: float = 0.03):
    """event_id ã® room_list ã‚’å…¨ãƒšãƒ¼ã‚¸å–å¾—ã—ã¦ entries (list) ã‚’è¿”ã™"""
    entries = []
    page = 1
    while page <= max_pages:
        params = {"event_id": event_id, "p": page}
        data = http_get_json(API_ROOM_LIST, params=params, retries=3, timeout=12)
        if not data:
            break
        # ãƒšãƒ¼ã‚¸å†…ã® list ã‚’å–å¾—
        page_entries = data.get("list") or []
        if page_entries:
            entries.extend(page_entries)
        # last_page ãŒã‚ã‹ã‚Œã°ãã“ã¾ã§å›ã™
        last_page = data.get("last_page")
        current_page = data.get("current_page") or page
        try:
            current_page_i = int(current_page)
        except Exception:
            current_page_i = page
        if last_page is not None:
            try:
                last_page_i = int(last_page)
                if current_page_i >= last_page_i:
                    break
                else:
                    page += 1
                    time.sleep(sleep_between_pages)
                    continue
            except Exception:
                # last_page ãŒæ•°å­—åŒ–ã§ããªã„å ´åˆã¯ next_page ã‚’ä½¿ã†
                pass
        # next_page ãŒ None ãªã‚‰çµ‚ç«¯
        if data.get("next_page") is None:
            break
        # safety increment
        page += 1
        time.sleep(sleep_between_pages)
    return entries

# ---------- contribution_ranking ã‚’è¤‡æ•°ã® room_id ã§è©¦ã™ ----------
def fetch_event_detail_try_many(event_id: int, candidate_room_ids: List[str], max_attempts_per_room: int = 2):
    """
    matched_entries ã® room_id ã‚’é †ã«è©¦ã—ã€event æƒ…å ±ãŒè¿”ã‚‹ã¾ã§ç¶šã‘ã‚‹ã€‚
    æˆ»ã‚Šå€¤: dict (å¯èƒ½ãªé™ã‚Š event_name/started_at/ended_at/event_url/image ã‚’å«ã‚€)
    """
    for rid in candidate_room_ids:
        params = {"event_id": event_id, "room_id": rid}
        data = http_get_json(API_CONTRIBUTION, params=params, retries=max_attempts_per_room, timeout=10)
        if not data:
            continue
        # data["event"] ãŒã‚ã‚‹ã‹
        if isinstance(data, dict) and "event" in data and isinstance(data["event"], dict):
            ev = data["event"]
            return {
                "event_name": ev.get("event_name"),
                "started_at": ev.get("started_at"),
                "ended_at": ev.get("ended_at"),
                "event_url": ev.get("event_url"),
                "event_image": ev.get("image")
            }
    return {}

# ---------- 1ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†ã™ã‚‹ï¼ˆç®¡ç†ãƒ«ãƒ¼ãƒ ã¨ç…§åˆã—ã¦ãƒ¬ã‚³ãƒ¼ãƒ‰ç”Ÿæˆï¼‰ ----------
def process_event(event_id: int, managed_rooms_df: pd.DataFrame, archive_map: dict, sleep_between_requests: float = 0.02):
    """
    event_id ã‚’å‡¦ç†ã—ã€managed_rooms ã«å¯¾å¿œã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ list ã‚’è¿”ã™ã€‚
    - room_list ã‚’å…¨éƒ¨å–ã‚‹
    - ç®¡ç†å¯¾è±¡ã® room_id ã¨ãƒãƒƒãƒã™ã‚‹ entries ã‚’æŠ½å‡º
    - ãƒãƒƒãƒãŒã‚ã‚Œã° contribution_ranking ã‚’å€™è£œ room_id ã§è©¦ã—ã€event info ã‚’å¾—ã‚‹
    - archive_map ã‚’ fallback ã«ä½¿ç”¨
    """
    recs = []
    entries = fetch_all_room_entries(event_id)
    if not entries:
        return recs

    # managed room id set (æ–‡å­—åˆ—)
    managed_ids = set(managed_rooms_df.index.astype(str).tolist())
    # å¯èƒ½ãªã‚‰ account_id ã‚‚æ¯”è¼ƒå¯¾è±¡ã«å…¥ã‚Œã‚‹ (è£œåŠ©)
    account_map = {}
    if "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID" in managed_rooms_df.columns:
        account_map = {str(v): idx for idx, v in managed_rooms_df["ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"].items() if pd.notna(v)}

    matched_entries = []
    for e in entries:
        rid = str(e.get("room_id")) if e.get("room_id") is not None else ""
        if rid in managed_ids:
            matched_entries.append(e)
            continue
        # fallback: account_id match
        a = e.get("account_id")
        if a and str(a) in account_map:
            # locate room id from managed_rooms_df by account -> use that row's room id
            idx = account_map[str(a)]
            # idx is index label (ãƒ«ãƒ¼ãƒ ID) - ensure it's string
            # note: managed_rooms_df.index contains room_ids, but account_map values are index labels; careful
            # We'll still append if rid exists in the entry (prefers entry's room_id)
            matched_entries.append(e)

    if not matched_entries:
        return recs

    # Try to fetch event-level detail via contribution API by trying multiple room_id candidates
    candidate_rids = [str(e.get("room_id")) for e in matched_entries if e.get("room_id")]
    detail = fetch_event_detail_try_many(event_id, candidate_rids)
    # fallback to archive map
    if not detail:
        detail = archive_map.get(str(event_id), {})
        # if archived, remap keys: archive has started_at, ended_at maybe as ints or strings
        if detail:
            # ensure keys names consistent
            detail = {
                "event_name": detail.get("event_name"),
                "started_at": detail.get("started_at"),
                "ended_at": detail.get("ended_at"),
                "event_url": ( "https://www.showroom-live.com/event/" + detail.get("event_url_key") ) if detail.get("event_url_key") else None,
                "event_image": detail.get("image_m") or detail.get("image")
            }

    # For each matched entry, build record
    for e in matched_entries:
        rid = str(e.get("room_id")) if e.get("room_id") is not None else ""
        # rank/point/quest_level
        rank = e.get("rank") or e.get("position") or "-"
        point = e.get("point") or e.get("event_point") or e.get("total_point") or 0
        quest_level = None
        try:
            quest_level = e.get("event_entry", {}).get("quest_level")
        except Exception:
            quest_level = None
        if quest_level is None:
            quest_level = e.get("quest_level") or 0
        try:
            point = int(point)
        except Exception:
            try:
                point = int(float(point))
            except Exception:
                point = 0
        try:
            quest_level = int(quest_level)
        except Exception:
            quest_level = 0

        rec = {
            "PRå¯¾è±¡": "",
            "ãƒ©ã‚¤ãƒãƒ¼å": managed_rooms_df.loc[rid, "ãƒ«ãƒ¼ãƒ å"] if (rid in managed_rooms_df.index and "ãƒ«ãƒ¼ãƒ å" in managed_rooms_df.columns) else (e.get("room_name") or ""),
            "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID": managed_rooms_df.loc[rid, "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"] if (rid in managed_rooms_df.index and "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID" in managed_rooms_df.columns) else (e.get("account_id") or ""),
            "ã‚¤ãƒ™ãƒ³ãƒˆå": detail.get("event_name") or None,
            "é–‹å§‹æ—¥æ™‚": fmt_time(detail.get("started_at")),
            "çµ‚äº†æ—¥æ™‚": fmt_time(detail.get("ended_at")),
            "é †ä½": rank,
            "ãƒã‚¤ãƒ³ãƒˆ": point,
            "å‚™è€ƒ": "",
            "ç´ä»˜ã‘": "â—‹",
            "URL": detail.get("event_url") or "",
            "ãƒ¬ãƒ™ãƒ«": quest_level,
            "event_id": str(event_id),
            "ãƒ«ãƒ¼ãƒ ID": rid,
            "ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰": detail.get("event_image") or ""
        }
        recs.append(rec)
        # è»½ã„é…å»¶ã‚’å…¥ã‚Œã¦ API éè² è·ã‚’é¿ã‘ã‚‹
        time.sleep(sleep_between_requests)
    return recs

# ---------- ã‚¹ã‚­ãƒ£ãƒ³é–¢æ•°ï¼ˆæ”¹å–„ï¼‰ ----------
def check_event_has_any_page(event_id: int):
    """event_id ã® 1ãƒšãƒ¼ã‚¸ç›®ã‚’å©ã„ã¦ list ã‚„ total_entries, number_of_rooms ã‚’ç¢ºèª"""
    data = http_get_json(API_ROOM_LIST, params={"event_id": event_id, "p": 1}, retries=2, timeout=8)
    if not data:
        return False, None
    # åˆ¤å®š
    if isinstance(data, dict):
        if data.get("number_of_rooms") is not None:
            return int(data.get("number_of_rooms")) > 0, data
        if data.get("total_entries") is not None:
            try:
                return int(data.get("total_entries")) > 0, data
            except Exception:
                pass
        if "list" in data and isinstance(data["list"], list) and len(data["list"]) > 0:
            return True, data
    # default
    return False, data

def scan_event_ids(start_id: int, end_id: int, max_workers: int, progress_callback=None) -> List[int]:
    """æŒ‡å®šç¯„å›²ã‚’ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦æœ‰åŠ¹ event_id ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    ids = list(range(start_id, end_id + 1))
    valid = []
    total = len(ids)
    checked = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=max(2, min(max_workers, 30))) as ex:
        futures = {ex.submit(check_event_has_any_page, eid): eid for eid in ids}
        for fut in concurrent.futures.as_completed(futures):
            eid = futures[fut]
            checked += 1
            ok = False
            try:
                ok, data = fut.result()
            except Exception:
                ok = False
            if ok:
                valid.append(eid)
            if progress_callback:
                progress_callback(checked, total)
    return sorted(valid)

# ---------- ãƒ¡ã‚¤ãƒ³åé›†ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆé€²æ—ãƒ»é€”ä¸­ä¿å­˜ã¤ãï¼‰ ----------
def fetch_and_build_database(event_start: int, event_end: int, max_workers: int, save_interval: int, save_path_ftp: str, sleep_between_requests: float = 0.02):
    st.info("ğŸ” ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹")
    pbar = st.progress(0)
    st_write = st.empty()
    managed_rooms = load_managed_rooms(ROOM_LIST_URL)
    st.write(f"ç®¡ç†ãƒ«ãƒ¼ãƒ æ•°: {len(managed_rooms)}")
    archive_map = load_event_archive(ARCHIVE_URL)
    st.write(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¤ãƒ™ãƒ³ãƒˆæ•°(è£œå¡«ç”¨): {len(archive_map)}")

    # 1) ã‚¹ã‚­ãƒ£ãƒ³
    def scan_progress(checked, total):
        pbar.progress(int(checked/total*100))
        st_write.text(f"scan: {checked}/{total}")
    valid_ids = scan_event_ids(event_start, event_end, max_workers=max_workers, progress_callback=scan_progress)
    pbar.progress(0)
    st.write(f"æœ‰åŠ¹ã‚¤ãƒ™ãƒ³ãƒˆå€™è£œæ•°: {len(valid_ids)} (ç¯„å›² {event_start}ã€œ{event_end})")

    if not valid_ids:
        st.warning("æœ‰åŠ¹ã‚¤ãƒ™ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç¯„å›²ã‚„APIçŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()

    # 2) ä¸¦åˆ—ã§å„ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†
    total_events = len(valid_ids)
    processed = 0
    all_records = []
    saved_count = 0

    def save_partial(records, desc):
        if not records:
            return False
        df_tmp = pd.DataFrame(records)
        col_order = ["PRå¯¾è±¡","ãƒ©ã‚¤ãƒãƒ¼å","ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID","ã‚¤ãƒ™ãƒ³ãƒˆå","é–‹å§‹æ—¥æ™‚","çµ‚äº†æ—¥æ™‚",
                     "é †ä½","ãƒã‚¤ãƒ³ãƒˆ","å‚™è€ƒ","ç´ä»˜ã‘","URL","ãƒ¬ãƒ™ãƒ«","event_id","ãƒ«ãƒ¼ãƒ ID","ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰"]
        for c in col_order:
            if c not in df_tmp.columns:
                df_tmp[c] = ""
        df_tmp = df_tmp[col_order]
        csv_bytes = df_tmp.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        try:
            ftp_upload_bytes(save_path_ftp, csv_bytes, retries=2)
            st.success(f"é€”ä¸­ä¿å­˜: {desc} (records={len(records)})")
            return True
        except Exception as e:
            st.warning(f"FTPä¿å­˜å¤±æ•—: {e}ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ç”¨æ„ã—ã¾ã™ã€‚")
            st.download_button("é€”ä¸­ä¿å­˜ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name=f"event_db_partial_{int(time.time())}.csv")
            return False

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(process_event, eid, managed_rooms, archive_map, sleep_between_requests): eid for eid in valid_ids}
        for fut in concurrent.futures.as_completed(futures):
            eid = futures[fut]
            processed += 1
            try:
                recs = fut.result()
            except Exception as e:
                st.error(f"event_id={eid} å‡¦ç†ä¾‹å¤–: {e}")
                recs = []
            if recs:
                all_records.extend(recs)
            # é€²æ—æ›´æ–°
            pbar.progress(int(processed/total_events*100))
            st_write.text(f"å‡¦ç†: {processed}/{total_events} (collected={len(all_records)})")

            # é€”ä¸­ä¿å­˜: ä»¶æ•°ãƒ™ãƒ¼ã‚¹
            if save_interval > 0 and len(all_records) > 0 and (len(all_records) - saved_count) >= save_interval:
                ok = save_partial(all_records, f"{len(all_records)}ä»¶ / processed_events={processed}")
                if ok:
                    saved_count = len(all_records)

    # æœ€çµ‚æ•´å½¢
    if not all_records:
        st.info("å®Œäº†ã—ã¾ã—ãŸãŒã€ç®¡ç†å¯¾è±¡ãƒ«ãƒ¼ãƒ ã®ä¸€è‡´ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    df = pd.DataFrame(all_records)
    # ensure columns
    col_order = ["PRå¯¾è±¡","ãƒ©ã‚¤ãƒãƒ¼å","ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID","ã‚¤ãƒ™ãƒ³ãƒˆå","é–‹å§‹æ—¥æ™‚","çµ‚äº†æ—¥æ™‚",
                 "é †ä½","ãƒã‚¤ãƒ³ãƒˆ","å‚™è€ƒ","ç´ä»˜ã‘","URL","ãƒ¬ãƒ™ãƒ«","event_id","ãƒ«ãƒ¼ãƒ ID","ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰"]
    for c in col_order:
        if c not in df.columns:
            df[c] = ""
    df = df[col_order]
    # æ–°ã—ã„ event_id ãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«ã‚½ãƒ¼ãƒˆï¼ˆæ•°å€¤ã¨ã—ã¦ï¼‰
    try:
        df["event_id_num"] = df["event_id"].astype(int)
        df.sort_values("event_id_num", ascending=False, inplace=True)
        df.drop(columns=["event_id_num"], inplace=True)
    except Exception:
        pass

    # æœ€çµ‚ä¿å­˜
    csv_bytes = df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    try:
        ftp_upload_bytes(save_path_ftp, csv_bytes, retries=3)
        st.success(f"æœ€çµ‚ä¿å­˜: FTP ã«ä¿å­˜ã—ã¾ã—ãŸ ({save_path_ftp}) ä»¶æ•°={len(df)}")
    except Exception as e:
        st.warning(f"æœ€çµ‚FTPä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ç”¨æ„ã—ã¾ã™ã€‚")
        st.download_button("æœ€çµ‚CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name=f"event_database_{int(time.time())}.csv")

    return df

# ---------- UI ----------
st.set_page_config(page_title="SHOWROOM ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæ”¹å–„ç‰ˆï¼‰", layout="wide")
st.title("SHOWROOM ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæ”¹å–„ç‰ˆï¼‰")

with st.sidebar:
    st.header("è¨­å®š")
    EVENT_ID_START = st.number_input("EVENT_ID_START", value=40290, step=1)
    EVENT_ID_END = st.number_input("EVENT_ID_END", value=40310, step=1)
    MAX_WORKERS = st.number_input("MAX_WORKERS", min_value=1, max_value=50, value=2, step=1)
    SAVE_INTERVAL = st.number_input("SAVE_INTERVAL(ä»¶æ•°)", min_value=0, value=200, step=50)
    SAVE_PATH_FTP = st.text_input("FTP ä¿å­˜ãƒ‘ã‚¹", value="/mksoul-pro.com/showroom/file/event_database.csv")
    SLEEP_BETWEEN_REQUESTS = st.number_input("sleep_between_requests (ç§’)", min_value=0.0, value=0.02, step=0.01)
    st.markdown("FTP ã¯ st.secrets['ftp'] ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")

if st.button("å®Ÿè¡Œ: ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆã‚¹ã‚­ãƒ£ãƒ³â†’å–å¾—ï¼‰"):
    try:
        df_res = fetch_and_build_database(
            int(EVENT_ID_START),
            int(EVENT_ID_END),
            int(MAX_WORKERS),
            int(SAVE_INTERVAL),
            str(SAVE_PATH_FTP),
            float(SLEEP_BETWEEN_REQUESTS)
        )
        if isinstance(df_res, pd.DataFrame) and not df_res.empty:
            st.success(f"å®Œäº†: {len(df_res)} ä»¶")
            st.dataframe(df_res.head(200))
            st.download_button("CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=df_res.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"), file_name="event_database.csv")
        else:
            st.info("çµæœã¯ç©ºã§ã™ã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"è‡´å‘½ã‚¨ãƒ©ãƒ¼: {e}")
        st.exception(traceback.format_exc())
