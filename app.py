# app.py
import streamlit as st
import requests
import pandas as pd
import io
import time
import ftplib
import socket
import traceback
from datetime import datetime
import pytz
import concurrent.futures
from typing import List, Dict, Any

JST = pytz.timezone("Asia/Tokyo")

# ---------- è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ UI ã§ä¸Šæ›¸ãå¯èƒ½ã«ã—ã¦ã„ã¾ã™ï¼‰ ----------
API_ROOM_LIST = "https://www.showroom-live.com/api/event/room_list"
API_CONTRIBUTION = "https://www.showroom-live.com/api/event/contribution_ranking"
ROOM_LIST_URL = "https://mksoul-pro.com/showroom/file/room_list.csv"

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ FTPï¼ˆ**æ¨å¥¨ï¼šStreamlit Secrets ã«è¨­å®šã—ã¦ãã ã•ã„**ï¼‰
# st.secrets ã®ä¾‹:
# [ftp]
# host = "ftp11.gmoserver.jp"
# user = "sd0866487@gmoserver.jp"
# password = "v$p7d56C4#QpfCvj"
DEFAULT_FTP_FALLBACK = {
    "host": None,
    "user": None,
    "password": None
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; mksoul-bot/1.0)"
}

# ---------- ãƒ˜ãƒ«ãƒ‘ãƒ¼ ----------
def http_get_json(url: str, params: dict = None, retries: int = 2, timeout: float = 10.0, backoff: float = 0.5):
    """GET -> JSONï¼ˆå®‰å…¨ãƒ©ãƒƒãƒ—ï¼‰ã€‚å¤±æ•—æ™‚ã¯ None ã‚’è¿”ã™ã€‚"""
    for attempt in range(retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, params=params, timeout=timeout)
            if r.status_code == 200:
                try:
                    return r.json()
                except ValueError:
                    return None
            else:
                # 404 ç­‰ã¯ None
                return None
        except (requests.RequestException, socket.timeout):
            if attempt < retries:
                time.sleep(backoff * (attempt + 1))
                continue
            return None
    return None

def ftp_upload_bytes(file_path: str, content_bytes: bytes, retries: int = 3):
    """FTP ã«ãƒã‚¤ãƒˆåˆ—ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚Secretsã‹ã‚‰FTPæƒ…å ±å–å¾—ã€‚å¤±æ•—ã™ã‚‹ã¨ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ã€‚"""
    ftp_host = st.secrets.get("ftp", {}).get("host") or DEFAULT_FTP_FALLBACK["host"]
    ftp_user = st.secrets.get("ftp", {}).get("user") or DEFAULT_FTP_FALLBACK["user"]
    ftp_pass = st.secrets.get("ftp", {}).get("password") or DEFAULT_FTP_FALLBACK["password"]
    if not ftp_host or not ftp_user:
        raise RuntimeError("FTP information not found in st.secrets['ftp']. Please set ftp.host/ftp.user/ftp.password.")
    last_err = None
    for i in range(retries):
        try:
            with ftplib.FTP(ftp_host, timeout=30) as ftp:
                ftp.login(ftp_user, ftp_pass)
                with io.BytesIO(content_bytes) as bf:
                    bf.seek(0)
                    ftp.storbinary(f"STOR {file_path}", bf)
            return True
        except Exception as e:
            last_err = e
            time.sleep(1 + i)
    raise last_err

def fmt_time(ts):
    """Unixç§’ -> 'YYYY/MM/DD HH:MM' (JST) ã€‚ None -> '' """
    try:
        if ts is None:
            return ""
        ts = int(ts)
        # ãƒŸãƒªç§’å¯¾ç­–
        if ts > 20000000000:
            ts = ts // 1000
        dt = datetime.fromtimestamp(ts, JST)
        return dt.strftime("%Y/%m/%d %H:%M")
    except Exception:
        return ""

# ---------- ç®¡ç†ãƒ«ãƒ¼ãƒ ï¼ˆroom_list.csvï¼‰ã®å–å¾— ----------
@st.cache_data(ttl=3600)
def load_managed_rooms(url: str) -> pd.DataFrame:
    """
    ç®¡ç†ãƒ«ãƒ¼ãƒ CSVã‚’å–å¾—ã—ã¦ DataFrame ã«ã™ã‚‹ã€‚
    æŸ”è»Ÿã«ãƒ˜ãƒƒãƒ€æœ‰ç„¡ã‚’åˆ¤æ–­ã™ã‚‹ï¼ˆç°¡æ˜“ï¼‰ã€‚
    æœŸå¾…åˆ—ï¼ˆå¯èƒ½ï¼‰: ãƒ«ãƒ¼ãƒ ID, ãƒ«ãƒ¼ãƒ å, ãƒ«ãƒ¼ãƒ URL, ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID, æœ€çµ‚ãƒ©ã‚¤ãƒ–æ—¥æ™‚, ...
    æœ€çµ‚çš„ã« index ã‚’ 'ãƒ«ãƒ¼ãƒ ID' (æ–‡å­—åˆ—) ã«ã™ã‚‹ã€‚
    """
    r = requests.get(url, headers=HEADERS, timeout=15)
    r.raise_for_status()
    txt = r.content.decode("utf-8-sig")
    # ã¾ãšãƒ˜ãƒƒãƒ€ã‚ã‚Šã§è©¦ã™
    try:
        df = pd.read_csv(io.StringIO(txt))
        cols = df.columns.tolist()
        # åˆ¤å®š: 'ãƒ«ãƒ¼ãƒ ID' ã‚«ãƒ©ãƒ ãŒã‚ã‚Œã°OKã€‚ãªã‘ã‚Œã°ãƒ˜ãƒƒãƒ€ãªã—ã¨ã—ã¦å‡¦ç†ã™ã‚‹
        if any("ãƒ«ãƒ¼ãƒ " in str(c) and ("ID" in str(c) or "id" in str(c).lower()) for c in cols):
            # æ­£è¦åŒ– - åˆ—åã®ã‚ˆãã‚ã‚‹è‹±èª/æ—¥æœ¬èªã‚’å¤‰æ›ã—ã¦çµ±ä¸€
            rename_map = {}
            for c in cols:
                lc = str(c).lower()
                if "room" in lc and "id" in lc:
                    rename_map[c] = "ãƒ«ãƒ¼ãƒ ID"
                elif "name" in lc and ("room" in lc or "ãƒ«ãƒ¼ãƒ " in c):
                    rename_map[c] = "ãƒ«ãƒ¼ãƒ å"
                elif "account" in lc or "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ" in c:
                    rename_map[c] = "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"
                elif "url" in lc:
                    rename_map[c] = "ãƒ«ãƒ¼ãƒ URL"
            df.rename(columns=rename_map, inplace=True)
        else:
            # ãƒ˜ãƒƒãƒ€ã‚ã‚Šã ãŒæœŸå¾…åˆ—ãŒãªã„ -> fallthrough to headerless
            raise Exception("no expected headers")
    except Exception:
        # headerless: èª­ã¿ç›´ã—
        df = pd.read_csv(io.StringIO(txt), header=None)
        # æœ€ä½é™1åˆ—ã¯ã‚ã‚‹ã¯ãšã€‚æ¨å®šã§åˆ—ã‚’å‰²ã‚Šå½“ã¦ã‚‹
        # Aåˆ—: ãƒ«ãƒ¼ãƒ ID, Båˆ—: ãƒ«ãƒ¼ãƒ å, Cåˆ—: ãƒ«ãƒ¼ãƒ URL, Dåˆ—: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID, ...
        col_map = {}
        if df.shape[1] >= 1:
            col_map[0] = "ãƒ«ãƒ¼ãƒ ID"
        if df.shape[1] >= 2:
            col_map[1] = "ãƒ«ãƒ¼ãƒ å"
        if df.shape[1] >= 3:
            col_map[2] = "ãƒ«ãƒ¼ãƒ URL"
        if df.shape[1] >= 4:
            col_map[3] = "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"
        df.rename(columns=col_map, inplace=True)
    # ãƒ«ãƒ¼ãƒ ID ã‚’æ–‡å­—åˆ—åŒ–ã—ã¦ index ã‚’è¨­å®š
    if "ãƒ«ãƒ¼ãƒ ID" not in df.columns:
        # æœ€ä½é™ãƒ«ãƒ¼ãƒ IDåˆ—ãŒå–ã‚Œãªã„ã¨å›°ã‚‹ã€‚ç©ºDFè¿”ã™
        return pd.DataFrame(columns=["ãƒ«ãƒ¼ãƒ ID"])
    df["ãƒ«ãƒ¼ãƒ ID"] = df["ãƒ«ãƒ¼ãƒ ID"].astype(str).str.strip()
    df = df.set_index("ãƒ«ãƒ¼ãƒ ID", drop=False)
    return df

# ---------- ã‚¹ã‚­ãƒ£ãƒ³ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆevent_id ç¯„å›²ã‹ã‚‰ã€Œæœ‰åŠ¹ã€ã‚¤ãƒ™ãƒ³ãƒˆã‚’åˆ—æŒ™ï¼‰ ----------
def check_event_has_rooms(event_id: int, timeout: float = 8.0) -> bool:
    """event_id ã® 1ãƒšãƒ¼ã‚¸ç›®ã‚’å©ã„ã¦éƒ¨å±‹æ•°ãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’åˆ¤å®š"""
    params = {"event_id": event_id, "p": 1}
    data = http_get_json(API_ROOM_LIST, params=params, timeout=timeout, retries=1)
    if not data:
        return False
    # API ã®è¿”ã—æ–¹ãŒç’°å¢ƒã§é•ã†ã®ã§æŸ”è»Ÿã«åˆ¤å®š
    if isinstance(data, dict):
        # number_of_rooms or total_entries or list
        if data.get("number_of_rooms") is not None:
            return int(data.get("number_of_rooms")) > 0
        if data.get("total_entries") is not None:
            return int(data.get("total_entries")) > 0
        if "list" in data and isinstance(data["list"], list):
            return len(data["list"]) > 0
    return False

def scan_event_ids(start_id: int, end_id: int, max_workers: int, progress_callback=None) -> List[int]:
    """æŒ‡å®šç¯„å›²ã® event_id ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦æœ‰åŠ¹ãª ID ã®ã¿è¿”ã™"""
    event_ids = list(range(start_id, end_id + 1))
    valid = []
    total = len(event_ids)
    checked = 0

    # ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå°‘æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã§ï¼‰ã€‚
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(check_event_has_rooms, eid): eid for eid in event_ids}
        for fut in concurrent.futures.as_completed(futures):
            eid = futures[fut]
            checked += 1
            try:
                ok = fut.result()
            except Exception:
                ok = False
            if ok:
                valid.append(eid)
            if progress_callback:
                progress_callback(checked, total)
    return sorted(valid)

# ---------- ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ãƒ•ã‚§ãƒ¼ã‚º ----------
def fetch_all_room_entries(event_id: int, max_pages: int = 200, sleep_between_pages: float = 0.05):
    """event_id ã® room_list ã‚’å…¨ãƒšãƒ¼ã‚¸å–å¾—ã—ã¦ list ã‚’è¿”ã™"""
    entries = []
    page = 1
    while page <= max_pages:
        params = {"event_id": event_id, "p": page}
        data = http_get_json(API_ROOM_LIST, params=params, timeout=12, retries=1)
        if not data:
            break
        # list ã‚­ãƒ¼ã‚’æœŸå¾…
        page_entries = data.get("list") or []
        if not page_entries:
            break
        entries.extend(page_entries)
        # last_page / next_page ãŒã‚ã‚‹ãªã‚‰åˆ©ç”¨
        last_page = data.get("last_page")
        next_page = data.get("next_page")
        if last_page is not None:
            if page >= int(last_page):
                break
        if next_page is None:
            break
        page += 1
        time.sleep(sleep_between_pages)
    return entries

def fetch_event_detail_via_contrib(event_id: int, any_room_id: str):
    """contribution_ranking API ã‚’ 1 å›å©ã„ã¦ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ï¼ˆevent_name, started_at, ended_at, event_url, imageï¼‰ã‚’è¿”ã™"""
    params = {"event_id": event_id, "room_id": any_room_id}
    data = http_get_json(API_CONTRIBUTION, params=params, timeout=12, retries=1)
    if not data:
        return {}
    # data ã« 'event' ã‚­ãƒ¼ãŒã‚ã‚Œã°åˆ©ç”¨
    if isinstance(data, dict) and "event" in data and isinstance(data["event"], dict):
        ev = data["event"]
        return {
            "event_name": ev.get("event_name"),
            "started_at": ev.get("started_at"),
            "ended_at": ev.get("ended_at"),
            "event_url": ev.get("event_url"),
            "event_image": ev.get("image")
        }
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: None
    return {}

def process_event(event_id: int, managed_rooms_df: pd.DataFrame, sleep_between_requests: float = 0.02):
    """
    1 event ã®å‡¦ç†ã€‚ç®¡ç†ãƒ«ãƒ¼ãƒ ã«ãƒãƒƒãƒã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿è¿”ã™ï¼ˆlist of dictï¼‰ã€‚
    å„ dict ã¯æœ€çµ‚çš„ãª CSV ã® 1 è¡Œã«ãªã‚Šã¾ã™ã€‚
    """
    results = []
    entries = fetch_all_room_entries(event_id)
    if not entries:
        return results

    # å…¨ entries ã®ä¸­ã§ç®¡ç†å¯¾è±¡ã®ãƒ«ãƒ¼ãƒ IDã ã‘æŠ½å‡º
    managed_ids = set(managed_rooms_df.index.astype(str).tolist())
    matched_entries = [e for e in entries if str(e.get("room_id")) in managed_ids]
    if not matched_entries:
        return results  # ç®¡ç†å¯¾è±¡ãƒ«ãƒ¼ãƒ ãŒã„ãªã‘ã‚Œã°ç„¡è¦–

    # event-level detail: contribution API ã‚’ 1 å›ã ã‘å©ãï¼ˆç¬¬1å€™è£œã®ç®¡ç†å¯¾è±¡roomã§ï¼‰
    any_rid = str(matched_entries[0].get("room_id"))
    detail = fetch_event_detail_via_contrib(event_id, any_rid)
    # fallback: started_at/ended_at ãŒ entries å†…ã«ç„¡ã‘ã‚Œã° None ã®ã¾ã¾

    for ent in matched_entries:
        rid = str(ent.get("room_id"))
        # é †ä½ã€ãƒã‚¤ãƒ³ãƒˆã€quest_level ã®å–ã‚Šå‡ºã—ï¼ˆæŸ”è»Ÿã«ï¼‰
        rank = ent.get("rank") or ent.get("position") or ent.get("event_entry", {}).get("rank") or "-"
        point = ent.get("point") or ent.get("event_point") or ent.get("total_point") or 0
        # quest_level ã¯ event_entry å†…ã«ã‚ã‚‹å ´åˆãŒå¤šã„
        quest_level = ent.get("event_entry", {}).get("quest_level")
        if quest_level is None:
            quest_level = ent.get("quest_level") or 0
        # convert numeric
        try:
            point = int(point)
        except Exception:
            try:
                point = int(float(point))
            except Exception:
                point = 0
        try:
            quest_level = int(quest_level)
        except Exception:
            quest_level = 0

        rec = {
            "PRå¯¾è±¡": "",
            "ãƒ©ã‚¤ãƒãƒ¼å": managed_rooms_df.loc[rid, "ãƒ«ãƒ¼ãƒ å"] if rid in managed_rooms_df.index and "ãƒ«ãƒ¼ãƒ å" in managed_rooms_df.columns else "",
            "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID": managed_rooms_df.loc[rid, "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"] if rid in managed_rooms_df.index and "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID" in managed_rooms_df.columns else "",
            "ã‚¤ãƒ™ãƒ³ãƒˆå": detail.get("event_name") or ent.get("event_entry", {}).get("event_name") or None,
            "é–‹å§‹æ—¥æ™‚": fmt_time(detail.get("started_at") or ent.get("event_entry", {}).get("entried_at")),
            "çµ‚äº†æ—¥æ™‚": fmt_time(detail.get("ended_at") or ent.get("event_entry", {}).get("updated_at")),
            "é †ä½": rank,
            "ãƒã‚¤ãƒ³ãƒˆ": point,
            "å‚™è€ƒ": "",
            "ç´ä»˜ã‘": "â—‹",
            "URL": detail.get("event_url") or "",
            "ãƒ¬ãƒ™ãƒ«": quest_level,
            "event_id": str(event_id),
            "ãƒ«ãƒ¼ãƒ ID": rid,
            "ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰": detail.get("event_image") or ""
        }
        results.append(rec)
        time.sleep(sleep_between_requests)
    return results

# ---------- ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆä¸¦åˆ— + é€²æ— + é€”ä¸­ä¿å­˜ï¼‰ ----------
def fetch_and_build_database(event_start: int, event_end: int, max_workers: int, save_interval: int, save_path_ftp: str, sleep_between_requests: float = 0.02):
    """
    - ã‚¹ã‚­ãƒ£ãƒ³ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆæœ‰åŠ¹ event_id ã®æŠ½å‡ºï¼‰
    - ä¸¦åˆ—ã§ event ã”ã¨ã®å‡¦ç†
    - çµæœã‚’ CSV ã«ã—ã¦ FTP ã¸éƒ½åº¦ä¿å­˜ï¼ˆsave_intervalï¼‰
    - æœ€çµ‚ DataFrame ã‚’è¿”ã™ï¼ˆpandas.DataFrameï¼‰
    """
    st.info("ğŸ“¡ ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§ï¼ˆIDãƒ¬ãƒ³ã‚¸ï¼‰ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã„ã¾ã™...")
    progress_text = st.empty()
    pbar = st.progress(0)

    # load managed rooms
    try:
        managed_rooms = load_managed_rooms(ROOM_LIST_URL)
    except Exception as e:
        st.error(f"ç®¡ç†ãƒ«ãƒ¼ãƒ ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return pd.DataFrame()

    # 1) ã‚¹ã‚­ãƒ£ãƒ³: æœ‰åŠ¹ event_id æŠ½å‡º
    valid_event_ids = []
    def scan_progress(checked, total):
        pbar.progress(int(checked / total * 100))
        progress_text.text(f"ã‚¹ã‚­ãƒ£ãƒ³: {checked}/{total}")
    valid_event_ids = scan_event_ids(event_start, event_end, max_workers=max(2, min(max_workers, 20)), progress_callback=scan_progress)
    pbar.progress(0)
    progress_text.text(f"ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†: æœ‰åŠ¹ã‚¤ãƒ™ãƒ³ãƒˆ {len(valid_event_ids)} ä»¶")

    if not valid_event_ids:
        st.warning("æŒ‡å®šç¯„å›²å†…ã«æœ‰åŠ¹ãªã‚¤ãƒ™ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    # 2) ä¸¦åˆ—å‡¦ç†: å„ event ã‚’å‡¦ç†ã—ã¦ç®¡ç†ãƒ«ãƒ¼ãƒ åˆ†ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
    st.info("âš™ï¸ æœ‰åŠ¹ã‚¤ãƒ™ãƒ³ãƒˆã‚’ä¸¦åˆ—ã§å‡¦ç†ã—ã¦ã„ã¾ã™...")
    total_events = len(valid_event_ids)
    processed_events = 0
    all_records = []

    # helper for saving intermediate CSV
    def save_progress_csv(records: List[dict], desc: str):
        if not records:
            return False
        df_tmp = pd.DataFrame(records)
        # final column order
        col_order = ["PRå¯¾è±¡", "ãƒ©ã‚¤ãƒãƒ¼å", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ã‚¤ãƒ™ãƒ³ãƒˆå", "é–‹å§‹æ—¥æ™‚", "çµ‚äº†æ—¥æ™‚",
                     "é †ä½", "ãƒã‚¤ãƒ³ãƒˆ", "å‚™è€ƒ", "ç´ä»˜ã‘", "URL", "ãƒ¬ãƒ™ãƒ«", "event_id", "ãƒ«ãƒ¼ãƒ ID", "ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰"]
        for c in col_order:
            if c not in df_tmp.columns:
                df_tmp[c] = ""
        df_tmp = df_tmp[col_order]
        csv_bytes = df_tmp.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        # attempt FTP upload, fallback to local download via streamlit
        try:
            ftp_upload_bytes(save_path_ftp, csv_bytes, retries=2)
            st.success(f"ğŸ’¾ é€”ä¸­ä¿å­˜å®Œäº† ({desc})")
            return True
        except Exception as e:
            st.warning(f"FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ{e}ï¼‰ã€‚ä»£ã‚ã‚Šã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚")
            st.download_button("é€”ä¸­ä¿å­˜CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name=f"event_db_partial_{int(time.time())}.csv")
            return False

    # Thread pool for event processing
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(process_event, eid, managed_rooms, sleep_between_requests): eid for eid in valid_event_ids}
        for fut in concurrent.futures.as_completed(futures):
            eid = futures[fut]
            processed_events += 1
            try:
                recs = fut.result()
            except Exception as e:
                # log and continue
                st.error(f"event_id={eid} ã®å‡¦ç†ã§ä¾‹å¤–: {e}")
                recs = []
            if recs:
                all_records.extend(recs)

            # é€²æ—è¡¨ç¤ºæ›´æ–°
            pct = int(processed_events / total_events * 100)
            pbar.progress(min(100, pct))
            progress_text.text(f"å‡¦ç†ä¸­: {processed_events}/{total_events} (event_id={eid})  â€” é›†è¨ˆä¸­ãƒ¬ã‚³ãƒ¼ãƒ‰: {len(all_records)}")

            # é€”ä¸­ä¿å­˜ãƒˆãƒªã‚¬ãƒ¼: SAVE_INTERVAL ã”ã¨ã«ä¿å­˜
            if save_interval > 0 and len(all_records) > 0 and (len(all_records) % save_interval == 0):
                try:
                    save_progress_csv(all_records, f"{len(all_records)}/{len(valid_event_ids)}")
                except Exception as e:
                    st.warning(f"é€”ä¸­ä¿å­˜ã§ã‚¨ãƒ©ãƒ¼: {e}")

    # æœ€çµ‚æ•´å½¢
    if not all_records:
        st.info("å‡¦ç†ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ç®¡ç†å¯¾è±¡ãƒ«ãƒ¼ãƒ ã®ãƒãƒƒãƒã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    df = pd.DataFrame(all_records)
    # ensure columns/order
    col_order = ["PRå¯¾è±¡", "ãƒ©ã‚¤ãƒãƒ¼å", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ã‚¤ãƒ™ãƒ³ãƒˆå", "é–‹å§‹æ—¥æ™‚", "çµ‚äº†æ—¥æ™‚",
                 "é †ä½", "ãƒã‚¤ãƒ³ãƒˆ", "å‚™è€ƒ", "ç´ä»˜ã‘", "URL", "ãƒ¬ãƒ™ãƒ«", "event_id", "ãƒ«ãƒ¼ãƒ ID", "ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰"]
    for c in col_order:
        if c not in df.columns:
            df[c] = ""
    df = df[col_order]
    # sort event_id desc (æ–°ã—ã„ã‚‚ã®ãŒä¸Š)
    try:
        df["event_id_sort"] = df["event_id"].astype(int)
        df.sort_values("event_id_sort", ascending=False, inplace=True)
        df.drop(columns=["event_id_sort"], inplace=True)
    except Exception:
        pass

    # æœ€çµ‚ä¿å­˜ï¼ˆFTP ã¸ï¼‰
    csv_bytes_final = df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    try:
        ftp_upload_bytes(save_path_ftp, csv_bytes_final, retries=3)
        st.success(f"âœ… æœ€çµ‚ä¿å­˜ï¼ˆFTPï¼‰å®Œäº†: {save_path_ftp} ï¼ˆ{len(df)} ä»¶ï¼‰")
    except Exception as e:
        st.warning(f"FTPä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚æœ€çµ‚CSV ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
        st.download_button("æœ€çµ‚çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes_final, file_name=f"event_database_{int(time.time())}.csv")

    return df

# ---------- Streamlit UI ----------
st.set_page_config(page_title="SHOWROOM ã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°", layout="wide")
st.title("ğŸ¯ SHOWROOM ã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°")

with st.sidebar:
    st.header("å®Ÿè¡Œè¨­å®š")
    EVENT_ID_START = st.number_input("EVENT_ID_START", value=33000, step=1)
    EVENT_ID_END = st.number_input("EVENT_ID_END", value=41000, step=1)
    MAX_WORKERS = st.number_input("MAX_WORKERS (ä¸¦åˆ—æ•°)", min_value=1, max_value=50, value=10, step=1)
    SAVE_INTERVAL = st.number_input("SAVE_INTERVALï¼ˆä»¶æ•°ï¼‰: é€”ä¸­ä¿å­˜ã‚’ã“ã®ä»¶æ•°ã”ã¨ã«å®Ÿæ–½", min_value=0, value=500, step=50)
    SAVE_PATH_FTP = st.text_input("FTP ä¿å­˜ãƒ‘ã‚¹(ä¾‹: /mksoul-pro.com/showroom/file/event_database.csv)", value="/mksoul-pro.com/showroom/file/event_database.csv")
    SLEEP_BETWEEN_REQUESTS = st.number_input("requesté–“ã‚¹ãƒªãƒ¼ãƒ—ï¼ˆç§’ã€APIè² è·å¯¾ç­–ï¼‰", min_value=0.0, value=0.02, step=0.01)
    st.markdown("---")
    st.markdown("**æ³¨æ„**: FTPæƒ…å ±ã¯ Streamlit Secrets ã« `ftp.host` / `ftp.user` / `ftp.password` ã‚’ã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
    if st.button("å®Ÿè¡Œï¼ˆã‚¹ã‚­ãƒ£ãƒ³â†’åé›†ï¼‰"):
        st.session_state.run_start = time.time()
        try:
            df_res = fetch_and_build_database(
                int(EVENT_ID_START),
                int(EVENT_ID_END),
                int(MAX_WORKERS),
                int(SAVE_INTERVAL),
                str(SAVE_PATH_FTP),
                float(SLEEP_BETWEEN_REQUESTS)
            )
            if isinstance(df_res, pd.DataFrame) and not df_res.empty:
                st.success(f"å‡¦ç†å®Œäº†: {len(df_res)} ä»¶ã‚’åé›†ã—ã¾ã—ãŸã€‚")
                st.dataframe(df_res.head(200))
                csv_bytes = df_res.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
                st.download_button("CSV ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name="event_database.csv")
            else:
                st.info("çµæœãŒç©ºã§ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            st.error(f"å®Ÿè¡Œä¸­ã«è‡´å‘½ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.exception(traceback.format_exc())
