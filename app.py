import streamlit as st
import pandas as pd
import requests
import time
import concurrent.futures
import pytz
from datetime import datetime
import io
import ftplib

# === è¨­å®š ===
JST = pytz.timezone("Asia/Tokyo")

ROOM_LIST_URL = "https://mksoul-pro.com/showroom/file/room_list.csv"
FTP_FILE_PATH = "/mksoul-pro.com/showroom/file/event_database.csv"

EVENT_ID_START = 30000
EVENT_ID_END = 30500  # âš™ï¸ãƒ†ã‚¹ãƒˆç¯„å›²ã‚’æŒ‡å®šï¼ˆåºƒã’ã‚‹å ´åˆã¯æ®µéšçš„ã«ï¼‰
MAX_WORKERS = 10  # ä¸¦åˆ—ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
SAVE_INTERVAL = 20  # 20ã‚¤ãƒ™ãƒ³ãƒˆã”ã¨ã«é€”ä¸­ä¿å­˜

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
}

# === FTPæ¥ç¶šç”¨ ===
def ftp_upload(file_path, content_bytes):
    ftp_host = st.secrets["ftp"]["host"]
    ftp_user = st.secrets["ftp"]["user"]
    ftp_pass = st.secrets["ftp"]["password"]
    with ftplib.FTP(ftp_host) as ftp:
        ftp.login(ftp_user, ftp_pass)
        with io.BytesIO(content_bytes) as f:
            ftp.storbinary(f"STOR {file_path}", f)

def ftp_download(file_path):
    ftp_host = st.secrets["ftp"]["host"]
    ftp_user = st.secrets["ftp"]["user"]
    ftp_pass = st.secrets["ftp"]["password"]
    with ftplib.FTP(ftp_host) as ftp:
        ftp.login(ftp_user, ftp_pass)
        buffer = io.BytesIO()
        try:
            ftp.retrbinary(f"RETR {file_path}", buffer.write)
            buffer.seek(0)
            return buffer.getvalue().decode("utf-8-sig")
        except Exception:
            return None


# === ãƒ‡ãƒ¼ã‚¿å–å¾—ç³»é–¢æ•° ===
def fetch_room_list_for_event(event_id):
    """ã‚¤ãƒ™ãƒ³ãƒˆã«å‚åŠ ã—ã¦ã„ã‚‹ãƒ«ãƒ¼ãƒ ã‚’å–å¾—"""
    all_rooms = []
    for page in range(1, 31):  # æœ€å¤§30ãƒšãƒ¼ã‚¸ï¼ˆç´„900ä»¶ï¼‰
        url = f"https://www.showroom-live.com/api/event/room_list?event_id={event_id}&p={page}"
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            if res.status_code != 200:
                break
            data = res.json()
            rooms = data.get("list", [])
            if not rooms:
                break
            all_rooms.extend(rooms)
            time.sleep(0.05)
        except Exception:
            break
    return all_rooms


def fetch_event_detail(event_id, room_id):
    """ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ã‚’å–å¾—ï¼ˆcontribution_ranking APIï¼‰"""
    url = f"https://www.showroom-live.com/api/event/contribution_ranking?event_id={event_id}&room_id={room_id}"
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        if res.status_code == 200:
            data = res.json()
            event = data.get("event", {})
            return {
                "event_name": event.get("event_name"),
                "started_at": event.get("started_at"),
                "ended_at": event.get("ended_at"),
                "event_url": event.get("event_url"),
                "event_image": event.get("image"),
            }
    except Exception:
        pass
    return {}


# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
def fetch_and_merge_event_data():
    # ç®¡ç†ãƒ«ãƒ¼ãƒ ãƒªã‚¹ãƒˆèª­è¾¼
    df_rooms = pd.read_csv(ROOM_LIST_URL, dtype=str)
    df_rooms["ãƒ«ãƒ¼ãƒ ID"] = df_rooms["ãƒ«ãƒ¼ãƒ ID"].astype(str)
    managed_rooms = df_rooms.set_index("ãƒ«ãƒ¼ãƒ ID")

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å–å¾—
    existing_csv = ftp_download(FTP_FILE_PATH)
    if existing_csv:
        df_existing = pd.read_csv(io.StringIO(existing_csv), dtype=str)
    else:
        df_existing = pd.DataFrame()

    all_records = []
    event_ids = list(range(EVENT_ID_START, EVENT_ID_END + 1))
    progress = st.progress(0)
    total = len(event_ids)

    def process_event(event_id):
        event_records = []
        room_list = fetch_room_list_for_event(event_id)
        for r in room_list:
            rid = str(r.get("room_id"))
            if rid not in managed_rooms.index:
                continue
            entry = r.get("event_entry", {})
            rank = r.get("rank") or "-"
            point = r.get("point") or 0
            quest_level = entry.get("quest_level", 0)
            detail = fetch_event_detail(event_id, rid)
            if not detail:
                continue

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ—¥ä»˜å¤‰æ›
            def fmt_time(ts):
                if not ts:
                    return ""
                try:
                    return datetime.fromtimestamp(int(ts), JST).strftime("%Y/%m/%d %H:%M")
                except Exception:
                    return ""

            rec = {
                "PRå¯¾è±¡": "",
                "ãƒ©ã‚¤ãƒãƒ¼å": managed_rooms.loc[rid, "ãƒ«ãƒ¼ãƒ å"],
                "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID": managed_rooms.loc[rid, "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID"],
                "ã‚¤ãƒ™ãƒ³ãƒˆå": detail.get("event_name"),
                "é–‹å§‹æ—¥æ™‚": fmt_time(detail.get("started_at")),
                "çµ‚äº†æ—¥æ™‚": fmt_time(detail.get("ended_at")),
                "é †ä½": rank,
                "ãƒã‚¤ãƒ³ãƒˆ": point,
                "ç´ä»˜ã‘": "â—‹",
                "URL": detail.get("event_url"),
                "ãƒ¬ãƒ™ãƒ«": quest_level,
                "event_id": str(event_id),
                "ãƒ«ãƒ¼ãƒ ID": rid,
                "ã‚¤ãƒ™ãƒ³ãƒˆç”»åƒï¼ˆURLï¼‰": detail.get("event_image"),
            }
            event_records.append(rec)
        return event_records

    # ä¸¦åˆ—å®Ÿè¡Œ
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_id = {executor.submit(process_event, eid): eid for eid in event_ids}
        for i, future in enumerate(concurrent.futures.as_completed(future_to_id)):
            try:
                eid = future_to_id[future]
                records = future.result()
                all_records.extend(records)
            except Exception:
                pass

            # é€²æ—æ›´æ–°
            progress.progress((i + 1) / total)

            # é€”ä¸­ä¿å­˜ï¼ˆ20ä»¶ã”ã¨ï¼‰
            if (i + 1) % SAVE_INTERVAL == 0 and all_records:
                df_partial = pd.DataFrame(all_records)
                merged = pd.concat([df_existing, df_partial], ignore_index=True)
                merged.drop_duplicates(subset=["event_id", "ãƒ«ãƒ¼ãƒ ID"], keep="last", inplace=True)
                merged.sort_values("event_id", ascending=False, inplace=True)
                csv_bytes = merged.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
                ftp_upload(FTP_FILE_PATH, csv_bytes)
                st.info(f"ğŸ’¾ é€”ä¸­ä¿å­˜å®Œäº† ({i+1}/{total})")

    # å®Œå…¨ä¿å­˜
    if all_records:
        df_new = pd.DataFrame(all_records)
        merged = pd.concat([df_existing, df_new], ignore_index=True)
        merged.drop_duplicates(subset=["event_id", "ãƒ«ãƒ¼ãƒ ID"], keep="last", inplace=True)
        merged.sort_values("event_id", ascending=False, inplace=True)
        csv_bytes = merged.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        ftp_upload(FTP_FILE_PATH, csv_bytes)
        st.success("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†ï¼")

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
        st.download_button(
            label="ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼‰",
            data=csv_bytes,
            file_name=f"event_database_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

    else:
        st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")


# === Streamlitç”»é¢ ===
def main():
    st.title("ğŸ¯ SHOWROOM ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ãƒ„ãƒ¼ãƒ«")
    st.caption("ä¸¦åˆ—å–å¾—ãƒ»é€²æ—è¡¨ç¤ºãƒ»é€”ä¸­ä¿å­˜å¯¾å¿œç‰ˆ")

    if st.button("ğŸš€ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ï¼ˆå®Ÿè¡Œï¼‰"):
        fetch_and_merge_event_data()


if __name__ == "__main__":
    main()